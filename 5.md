# Sequential Documentation: Python Sentinel-1 SAR Data Processing System

This documentation follows the **actual execution order** - starting from the user entry point and tracing through each file as it's called during normal operation.

## Entry Point: Package Initialization (__init__.py)

### System Starting Point

**Step 1: Package Import and Exposure**
```python
"""Sentinel-1 decoder package."""

from . import constants, utilities
from .l0decoder import Level0Decoder
from .l0file import Level0File

__version__ = "0.1"

__all__ = [
    "Level0Decoder",
    "Level0File", 
    "utilities",
    "constants",
]
```
- **User entry point**: When user imports the package, this file executes first
- **Public API definition**: Exposes main classes `Level0File` and `Level0Decoder`
- **Module organization**: Makes supporting modules available as submodules
- **Version tracking**: Defines package version for compatibility

**Step 2: Typical User Import**
```python
# User code typically starts like this:
from sentinel1decoder import Level0File
l0_file = Level0File("path/to/sentinel1_data.dat")
```
- **Simple interface**: User only needs to import `Level0File` for most use cases
- **File-based initialization**: User provides path to Sentinel-1 raw data file

---

## Primary Interface: Level0File Creation (l0file.py)

### Level0File Class Initialization - First File Called

**Step 3: Level0File Constructor Execution**
```python
class Level0File:
    def __init__(self, filename: str) -> None:
        self._filename = filename
        self._decoder = Level0Decoder(filename)  # ← CALLS l0decoder.py
```
- **Constructor triggered**: When user creates `Level0File("filename.dat")`
- **Decoder creation**: Immediately creates `Level0Decoder` instance
- **File path storage**: Preserves filename for later operations

**Step 4: Automatic Metadata Processing**
```python
        # Split metadata into blocks of consecutive packets w/ const swath number
        self._packet_metadata = self._index_df_on_bursts(self._decoder.decode_metadata())
        #                                                              ↑
        #                                                    CALLS l0decoder.py
```
- **Immediate processing**: Constructor automatically processes all packet headers
- **Burst organization**: Groups packets into processing blocks
- **Metadata extraction**: Calls decoder to read all headers from file

**Step 5: Lazy Loading Setup**
```python
        # Only calculate ephemeris if requested
        self._ephemeris: Optional[pd.DataFrame] = None
        
        # Only decode radar echoes from bursts if that data is requested
        self._burst_data_dict: Dict[int, Optional[np.ndarray]] = dict.fromkeys(
            self._packet_metadata.index.unique(level=c.BURST_NUM_FIELD_NAME), None
        )
```
- **Performance optimization**: Actual radar data only decoded when requested
- **Memory management**: Avoids loading large datasets unnecessarily

**Step 6: Burst Indexing Process**
```python
    def _index_df_on_bursts(self, packet_metadata: pd.DataFrame) -> pd.DataFrame:
        packet_metadata = packet_metadata.groupby(
            packet_metadata[[c.SWATH_NUM_FIELD_NAME, c.NUM_QUADS_FIELD_NAME]].diff().ne(0).any(axis=1).cumsum(),
            group_keys=True,
        ).apply(lambda x: x)
```
- **Configuration change detection**: Identifies when radar settings change
- **Burst boundary creation**: Groups consecutive packets with identical settings
- **Pandas operations**: Uses efficient DataFrame operations for grouping

---

## Core Decoder: Level0Decoder Operations (l0decoder.py) 

### Level0Decoder Class - Second File Called

**Step 7: Level0Decoder Constructor**
```python
class Level0Decoder:
    def __init__(self, filename: str, log_level: int = logging.WARNING):
        logging.basicConfig(filename="output_log.log", level=log_level)
        self.filename = filename
```
- **Called by Level0File**: Constructor triggered from Level0File.__init__
- **Logging setup**: Configures logging for debugging and error tracking
- **Simple initialization**: Just stores filename, no heavy processing yet

**Step 8: Metadata Decoding Process**
```python
    def decode_metadata(self) -> pd.DataFrame:
        output_row_list = []
        with open(self.filename, "rb") as f:
            while True:
                try:
                    output_dictionary_row, _ = self._read_single_packet(f)  # ← CALLS header parsing
                except NoMorePacketsException:
                    break
                output_row_list.append(output_dictionary_row)
        return pd.DataFrame(output_row_list)
```
- **Called by Level0File**: Triggered during Level0File construction
- **Sequential processing**: Reads file packet by packet from beginning to end
- **Header-only extraction**: Only processes headers, ignores payload data
- **DataFrame assembly**: Converts packet headers to structured DataFrame

**Step 9: Single Packet Reading**
```python
    def _read_single_packet(self, opened_file: BinaryIO) -> Tuple[dict, bytes]:
        # PACKET PRIMARY HEADER (6 bytes)
        data_buffer = opened_file.read(6)
        if not data_buffer:
            raise NoMorePacketsException()
        
        output_dictionary_row = hdrs.decode_primary_header(data_buffer)  # ← CALLS _headers.py
```
- **Binary file reading**: Reads exact byte sequences from file
- **Primary header processing**: First 6 bytes processed by header decoder
- **EOF detection**: Empty read indicates end of file

**Step 10: Secondary Header and Payload Handling**
```python
        # PACKET DATA FIELD (between 62 and 65534 bytes)
        pkt_data_len = output_dictionary_row[cnst.PACKET_DATA_LEN_FIELD_NAME]
        packet_data_buffer = opened_file.read(pkt_data_len)
        
        secondary_hdr = hdrs.decode_secondary_header(packet_data_buffer[:62])  # ← CALLS _headers.py
        output_dictionary_row.update(secondary_hdr)
        
        output_bytes = packet_data_buffer[62:]  # User data for later processing
        return output_dictionary_row, output_bytes
```
- **Variable-length reading**: Packet size determined from primary header
- **Secondary header processing**: Next 62 bytes contain detailed metadata  
- **Payload separation**: Remaining bytes are actual radar data
- **Return both parts**: Headers and payload returned to caller

---

## Header Processing: Packet Metadata Extraction (_headers.py)

### Header Decoding Functions - Third File Called

**Step 11: Primary Header Decoding**
```python
def decode_primary_header(header_bytes: bytes) -> dict:
    if not len(header_bytes) == 6:
        raise Exception(f"Primary header must be exactly 6 bytes. Received {len(header_bytes)} bytes.")
    
    tmp16 = int.from_bytes(header_bytes[:2], "big")
    packet_version_number = tmp16 >> 13  # Bit 0-2
    packet_type = (tmp16 >> 12) & 0x01  # Bit 3
    secondary_header_flag = (tmp16 >> 11) & 0x01  # Bit 4
    process_id = (tmp16 >> 4) & 0x7F  # Bit 5-11
    packet_category = tmp16 & 0xF  # Bit 12-15
```
- **Called by Level0Decoder**: Triggered for each packet during metadata scan
- **Bit field extraction**: Uses bit shifting and masking for precise field extraction
- **Network byte order**: "big" endian conversion from satellite data format
- **Validation**: Ensures exactly 6 bytes provided

**Step 12: Sequence and Length Processing**
```python
    tmp16 = int.from_bytes(header_bytes[2:4], "big")
    sequence_flags = tmp16 >> 14  # Bit 0-1
    packet_sequence_count = tmp16 & 0x3FFF  # Bit 2-15
    
    tmp16 = int.from_bytes(header_bytes[4:], "big")
    packet_data_length = tmp16 + 1  # Bit 0-15
```
- **Sequence tracking**: Manages packet ordering and potential fragmentation
- **Length calculation**: Add 1 per protocol specification
- **Payload size determination**: Used to read correct amount of data

**Step 13: Secondary Header Decoding**
```python
def decode_secondary_header(header_bytes: bytes) -> dict:
    if not len(header_bytes) == 62:
        raise Exception(f"Secondary header must be exactly 62 bytes. Received {len(header_bytes)} bytes.")
    
    # Datation service (6 bytes)
    coarse_time = int.from_bytes(header_bytes[:4], "big")
    fine_time = (int.from_bytes(header_bytes[4:6], "big") + 0.5) * (2 ** (-16))
```
- **Called by Level0Decoder**: Processes secondary header for each packet
- **Timing information**: Extracts GPS time when data was acquired
- **High precision timing**: Combines coarse and fine time for microsecond accuracy

**Step 14: Radar Configuration Extraction**
```python
    # Radar configuration support service (27 bytes)
    error_flag = header_bytes[31] >> 7  # Byte 31 Bit 0
    baq_mode = header_bytes[31] & 0x1F  # Byte 31 Bits 3-7
    signal_type = header_bytes[57] >> 4  # Byte 57 Bits 0-3
    swath_number = header_bytes[58]
    number_of_quads = int.from_bytes(header_bytes[59:61], "big")
```
- **Critical parameters**: BAQ mode determines compression algorithm
- **Signal type**: Distinguishes echo data (0) from calibration (>7)
- **Sample count**: Number of I/Q pairs in this packet's payload

**Step 15: Physical Parameter Calculations**
```python
    tmp16 = int.from_bytes(header_bytes[36:38], "big")
    txprr_sign = (-1) ** (1 - (tmp16 >> 15))
    txprr = txprr_sign * (tmp16 & 0x7FFF) * (cnst.F_REF**2) / (2**21)
    
    tmp24 = int.from_bytes(header_bytes[40:43], "big")
    tx_pulse_length = tmp24 / cnst.F_REF
```
- **Unit conversions**: Convert raw values to physical units using reference frequency
- **Sign handling**: Extract and apply sign bits for signed parameters
- **Radar parameters**: Pulse timing and frequency information for SAR processing

---

## Burst Data Request: Triggering Payload Decoding

### User Requests Burst Data - Triggering Deeper Processing

**Step 16: User Calls get_burst_data()**
```python
# User code continues:
burst_data = l0_file.get_burst_data(burst_number=1)
```
- **User trigger**: This call initiates actual payload processing
- **Until now**: Only headers have been processed, no radar data decoded

**Step 17: Burst Data Retrieval with Caching**
```python
def get_burst_data(self, burst: int, try_load_from_file: bool = True) -> np.ndarray:
    if self._burst_data_dict[burst] is None:
        if try_load_from_file:
            save_file_name = self._generate_burst_cache_filename(burst)
            try:
                self._burst_data_dict[burst] = np.load(save_file_name)
            finally:
                return self.get_burst_data(burst, try_load_from_file=False)
        else:
            self._burst_data_dict[burst] = self._decoder.decode_packets(self.get_burst_metadata(burst))
            #                                          ↑ 
            #                                    CALLS l0decoder.py again
```
- **Lazy loading**: Data only decoded when specifically requested
- **Cache checking**: Attempts to load from saved .npy file first
- **Fallback processing**: If cache fails, calls decoder to process raw data

**Step 18: Packet Decoding Dispatch**
```python
def decode_packets(self, input_header: pd.DataFrame) -> np.ndarray:
    # Validation and setup...
    
    while packet_counter < packets_to_process:
        this_header, packet_data_bytes = self._read_single_packet(f)
        
        if (this_header[cnst.SPACE_PACKET_COUNT_FIELD_NAME] 
            in input_header[cnst.SPACE_PACKET_COUNT_FIELD_NAME].values):
            
            baqmod = this_header[cnst.BAQ_MODE_FIELD_NAME]
            nq = this_header[cnst.NUM_QUADS_FIELD_NAME]
            data_decoder = UserDataDecoder(packet_data_bytes, baqmod, nq)  # ← CALLS _user_data_decoder.py
            this_data_packet = data_decoder.decode()
```
- **Selective processing**: Only processes packets in the requested burst
- **Header extraction**: Gets BAQ mode and sample count from headers
- **Decoder creation**: Creates UserDataDecoder for payload processing

---

## Data Decoding Dispatch: UserDataDecoder (_user_data_decoder.py)

### UserDataDecoder Class - Fourth File Called

**Step 19: UserDataDecoder Initialization**
```python
class UserDataDecoder:
    def __init__(self, data: bytes, baq_mode: int, num_quads: int) -> None:
        if baq_mode not in (0, 3, 4, 5, 12, 13, 14):
            logging.error(f"Unrecognized BAQ mode: {baq_mode}")
            raise Exception(f"Unrecognized BAQ mode: {baq_mode}")
        
        self.data = data
        self.baq_mode = baq_mode
        self.num_quads = num_quads
```
- **Called by Level0Decoder**: Created for each packet needing payload decoding
- **Mode validation**: Ensures BAQ mode is supported by the implementation
- **Parameter storage**: Stores payload bytes and decoding parameters

**Step 20: Decoding Method Dispatch**
```python
    def decode(self) -> List[complex]:
        if self.baq_mode == 0:
            # Bypass data is encoded as a simple list of 10-bit words.
            bypass_decoder = BypassDecoder(self.data, self.num_quads)  # ← CALLS _bypass_decoder.py
            IE = bypass_decoder.i_evens
            IO = bypass_decoder.i_odds
            QE = bypass_decoder.q_evens
            QO = bypass_decoder.q_odds
```
- **BAQ mode 0**: Uncompressed calibration data, uses BypassDecoder
- **Direct value access**: BypassDecoder processes all channels during initialization

**Step 21: FDBAQ Mode Processing**
```python
        elif self.baq_mode in (12, 13, 14):
            # FDBAQ data uses various types of Huffman encoding.
            scode_extractor = FDBAQDecoder(self.data, self.num_quads)  # ← CALLS _fdbaq_decoder.py
            brcs = scode_extractor.brcs
            thidxs = scode_extractor.thidxs
            
            # Reconstruct values using lookup tables
            IE = reconstruct_channel_vals(scode_extractor.s_ie, brcs, thidxs, self.num_quads)  # ← CALLS _sample_value_reconstruction.py
```
- **FDBAQ modes**: Compressed echo data, uses FDBAQDecoder
- **Two-stage process**: First Huffman decoding, then value reconstruction
- **Parameter extraction**: BRC and THIDX needed for reconstruction

**Step 22: Complex Sample Assembly**
```python
        # Re-order the even-indexed and odd-indexed sample channels here.
        decoded_data = []
        for i in range(len(IE)):
            decoded_data.append(complex(IE[i], QE[i]))  # Even sample
            decoded_data.append(complex(IO[i], QO[i]))  # Odd sample
        
        return decoded_data
```
- **Channel combination**: Four real channels become complex I/Q pairs
- **Interleaving**: Even and odd samples combined sequentially
- **Complex numbers**: Real=I channel, Imaginary=Q channel

---

## Branch A: Uncompressed Data Processing (_bypass_decoder.py)

### BypassDecoder Class - Fifth File Called (Path A)

**Step 23: BypassDecoder Initialization (if BAQ mode 0)**
```python
class BypassDecoder:
    def __init__(self, data: bytes, num_quads: int) -> None:
        self._data = data
        self._num_quads = num_quads
        
        _num_words = math.ceil((10 / 16) * num_quads)  # No. of 16-bit words per channel
        self._num_bytes = 2 * _num_words  # No. of 8-bit bytes per channel
        
        self._i_evens = self._process_channel(0)
        self._i_odds = self._process_channel(self._num_bytes)
        self._q_evens = self._process_channel(2 * self._num_bytes)
        self._q_odds = self._process_channel(3 * self._num_bytes)
```
- **Called by UserDataDecoder**: Only when BAQ mode is 0 (uncompressed)
- **Memory layout calculation**: 10-bit samples packed in byte stream
- **Automatic processing**: All four channels processed during construction
- **Sequential layout**: IE, IO, QE, QO channels stored consecutively

**Step 24: 10-Bit Sample Extraction**
```python
    def _process_channel(self, start_8bit_index: int) -> np.ndarray:
        index_8bit = start_8bit_index
        index_10bit = 0
        output_array = np.zeros(self._num_quads, dtype=int)
        
        while index_10bit < self._num_quads:
            # Extract first 10-bit value from 5-byte group
            s_code = (self._data[index_8bit] << 2 | self._data[index_8bit + 1] >> 6) & 1023
            output_array[index_10bit] = _ten_bit_unsigned_to_signed_int(s_code)
```
- **Bit-level extraction**: Combines parts of multiple bytes to form 10-bit values
- **Packing pattern**: 4 samples packed into every 5 bytes (40 bits)
- **Sign conversion**: Converts 10-bit two's complement to signed integers

**Step 25: Sign Conversion Function**
```python
def _ten_bit_unsigned_to_signed_int(ten_bit: int) -> int:
    # First bit is the sign, remaining 9 encode the number
    sign = int((-1) ** ((ten_bit >> 9) & 0x1))
    return sign * (ten_bit & 0x1FF)
```
- **Two's complement handling**: MSB indicates sign, lower 9 bits are magnitude
- **Sign calculation**: 0 → +1, 1 → -1 using power function
- **Value reconstruction**: Multiply magnitude by sign

---

## Branch B: Compressed Data Processing (_fdbaq_decoder.py)

### FDBAQDecoder Class - Fifth File Called (Path B)

**Step 26: FDBAQDecoder Initialization (if BAQ mode 12-14)**
```python
class FDBAQDecoder:
    def __init__(self, data: bytes, num_quads: int) -> None:
        self._bit_counter = 0
        self._byte_counter = 0
        self._data = data
        self._num_quads = num_quads
        self._num_baq_blocks = math.ceil(num_quads / 128)
        
        # Process all channels
        self._process_channel("IE", self._i_evens_scodes, read_brc=True)
        self._align_word_boundary()
        self._process_channel("IO", self._i_odds_scodes)
        # ...continuing for QE, QO
```
- **Called by UserDataDecoder**: Only when BAQ mode is 12, 13, or 14
- **Bit-level tracking**: Maintains position within compressed bit stream
- **Block calculation**: 128 samples per compression block
- **Sequential processing**: IE, IO, QE, QO in specific order

**Step 27: Huffman Tree Definitions**
```python
_TREE_BRC_ZERO = (0, (1, (2, 3)))
_TREE_BRC_ONE = (0, (1, (2, (3, 4))))
_TREE_BRC_TWO = (0, (1, (2, (3, (4, (5, 6))))))
_TREE_BRC_THREE = ((0, 1), (2, (3, (4, (5, (6, (7, (8, 9))))))))
_TREE_BRC_FOUR = (
    (0, (1, 2)),
    ((3, 4), ((5, 6), (7, (8, (9, ((10, 11), ((12, 13), (14, 15)))))))),
)
```
- **Nested tuple structure**: Represents binary Huffman trees for decompression
- **Increasing complexity**: Higher BRC levels have more complex trees
- **Tree traversal**: 0 bit goes left, 1 bit goes right in tuple structure

**Step 28: Bit Reading Functions**
```python
    def _next_bit(self) -> int:
        bit = (self._data[self._byte_counter] >> (7 - self._bit_counter)) & 0x01
        self._bit_counter = (self._bit_counter + 1) % 8
        if self._bit_counter == 0:
            self._byte_counter += 1
        return bit
```
- **Single bit extraction**: Gets one bit from current position
- **MSB first**: Reads bits from most significant to least significant
- **Position tracking**: Advances through byte stream bit by bit

**Step 29: Block Parameter Reading**
```python
    def _read_brc(self) -> int:
        residual = 0
        for i in range(3):
            residual = residual << 1
            residual += self._next_bit()
        return residual
        
    def _read_thidx(self) -> int:
        residual = 0
        for i in range(8):
            residual = residual << 1
            residual += self._next_bit()
        return residual
```
- **BRC extraction**: 3-bit compression level (0-4)
- **THIDX extraction**: 8-bit threshold index (0-255)
- **MSB-first construction**: Build integers bit by bit

**Step 30: Huffman Sample Decoding**
```python
    def _decode_sample(self, huffman_tree: HuffmanNode) -> SampleCode:
        sign = self._next_bit()
        current_node = huffman_tree
        while not isinstance(current_node, int):
            current_node = current_node[self._next_bit()]
            if current_node is None:
                raise ValueError("Invalid Huffman encoding")
        return SampleCode(sign, current_node)  # ← CREATES _sample_code.py object
```
- **Sign bit first**: Always starts with sign bit
- **Tree traversal**: Follow bit pattern through Huffman tree
- **Leaf detection**: Integer node indicates magnitude code found
- **SampleCode creation**: Stores sign and magnitude separately

---

## Sample Code Storage: SampleCode Data Structure (_sample_code.py)

### SampleCode Class - Sixth File Called

**Step 31: SampleCode Class Definition**
```python
class SampleCode:
    def __init__(self, sign: int, mcode: int) -> None:
        self._sign = sign
        self._mcode = mcode
        
    @property
    def sign(self) -> int:
        return self._sign
        
    @property  
    def mcode(self) -> int:
        return self._mcode
```
- **Called by FDBAQDecoder**: Created for each decoded Huffman sample
- **Immutable storage**: Sign and magnitude stored separately
- **Property access**: Read-only access to internal values
- **Data separation**: Prevents loss of sign information when magnitude is zero

**Step 32: Utility Methods**
```python
    def __repr__(self) -> str:
        return "SampleCode(%(sign)i, %(mcode)i)" % {"sign": self._sign, "mcode": self._mcode}
        
    def __eq__(self, other: object) -> bool:
        if not isinstance(other, SampleCode):
            return False
        return self._sign == other._sign and self._mcode == other._mcode
```
- **String representation**: Useful for debugging and logging
- **Equality comparison**: Enables testing and validation operations

---

## Value Reconstruction: Converting Codes to Amplitudes (_sample_value_reconstruction.py)

### Sample Value Reconstruction - Seventh File Called

**Step 33: Channel Reconstruction Function**
```python
def reconstruct_channel_vals(
    data: List[SampleCode],
    block_brcs: List[int], 
    block_thidxs: List[int],
    vals_to_process: int,
) -> np.ndarray:
    if not len(block_brcs) == len(block_thidxs):
        logging.error("Mismatched lengths of BRC block parameters")
    
    num_brc_blocks = len(block_brcs)
    out_vals = np.zeros(vals_to_process)
    n = 0
```
- **Called by UserDataDecoder**: For FDBAQ data after Huffman decoding
- **Parameter validation**: Ensures BRC and THIDX arrays match
- **Output allocation**: Pre-allocate NumPy array for results

**Step 34: Block-Based Processing**
```python
    # For each BRC block
    for block_index in range(num_brc_blocks):
        brc = int(block_brcs[block_index])
        thidx = int(block_thidxs[block_index])
        
        # For each code in the BRC block  
        for i in range(min(128, vals_to_process - n)):
            s_code = data[n]
```
- **Nested processing**: Outer loop for blocks, inner loop for samples
- **Parameter extraction**: Get compression parameters for current block
- **Boundary handling**: Final block may have fewer than 128 samples

**Step 35: BRC-Specific Reconstruction Logic**
```python
            if brc == 0:
                if thidx <= 3:
                    if s_code.mcode < 3:
                        out_vals[n] = (-1) ** s_code.sign * s_code.mcode
                    elif s_code.mcode == 3:
                        out_vals[n] = (-1) ** s_code.sign * lookup.b0[thidx]  # ← CALLS _lookup_tables.py
                else:
                    out_vals[n] = (-1) ** s_code.sign * lookup.nrl_b0[s_code.mcode] * lookup.sf[thidx]
```
- **BRC-dependent logic**: Different reconstruction methods for each compression level
- **Threshold-based switching**: THIDX determines direct vs. scaled reconstruction
- **Lookup table usage**: References tables for reconstruction values
- **Sign application**: Apply sign using power function

**Step 36: Progressive BRC Levels**
```python
            elif brc == 1:
                if thidx <= 3:
                    if s_code.mcode < 4:  # Higher threshold for BRC1
                        out_vals[n] = (-1) ** s_code.sign * s_code.mcode
                    elif s_code.mcode == 4:
                        out_vals[n] = (-1) ** s_code.sign * lookup.b1[thidx]
                else:
                    out_vals[n] = (-1) ** s_code.sign * lookup.nrl_b1[s_code.mcode] * lookup.sf[thidx]
            # Similar logic continues for brc == 2, 3, 4...
```
- **Increasing thresholds**: Higher BRC levels allow more direct magnitude codes
- **Dedicated tables**: Each BRC level has its own lookup tables
- **Consistent pattern**: Same reconstruction logic across all BRC levels

---

## Lookup Tables: Reconstruction Values (_lookup_tables.py)

### Reconstruction Lookup Tables - Eighth File Called

**Step 37: BRC Lookup Tables**
```python
# Table for simple reconstruction method, pg 78
b0 = [3.0, 3.0, 3.16, 3.53]
b1 = [4.0, 4.0, 4.08, 4.37]  
b2 = [6.0, 6.0, 6.0, 6.15, 6.5, 6.88]
b3 = [9.0, 9.0, 9.0, 9.0, 9.36, 9.50, 10.1]
b4 = [15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.22, 15.50, 16.05]
```
- **Called by reconstruction function**: Provides direct magnitude values
- **BRC-specific tables**: Each compression level has different base values
- **Increasing magnitudes**: Higher BRC levels have larger baseline values
- **Low-THIDX reconstruction**: Used when THIDX is below threshold

**Step 38: Normalized Reconstruction Levels**
```python
# Table of normalized reconstruction levels, pg 79
nrl_b0 = [0.3637, 1.0915, 1.8208, 2.6406]
nrl_b1 = [0.3042, 0.9127, 1.5216, 2.1313, 2.8426]
nrl_b2 = [0.2305, 0.6916, 1.1528, 1.6140, 2.0754, 2.5369, 3.1191]
# ...continuing for nrl_b3, nrl_b4
```
- **Scaling coefficients**: Multiplied by SF[THIDX] for final reconstruction
- **Progressive sizes**: Higher BRC levels have more reconstruction levels
- **High-THIDX reconstruction**: Used when THIDX exceeds threshold

**Step 39: Scaling Factor Table**
```python
# Table of sigma values (256 entries)
sf = [
    0.0, 0.630, 1.250, 1.880, 2.510, 3.130, 3.760, 4.390,
    # ...continuing for 256 total entries...
    253.490, 254.740, 255.990, 255.990,
]
```
- **Dynamic range control**: 256 scaling factors indexed by THIDX value
- **Generally increasing**: Provides larger dynamic range for higher THIDX
- **Final saturation**: Last value repeated to handle edge cases

---

## Supporting Infrastructure: Constants and Utilities

### Constants Definition (constants.py) - Used Throughout

**Step 40: Physical and System Constants**
```python
# Constant used to scale several data fields
F_REF = 37.53472224 * 1e6

# Useful for processing radar data
SPEED_OF_LIGHT_MPS = 299792458.0
TX_FREQ_HZ = 5.405e9
TX_WAVELENGTH_M = SPEED_OF_LIGHT_MPS / TX_FREQ_HZ
```
- **Referenced throughout**: Used by header parsing and utilities
- **Reference frequency**: Critical for time/frequency conversions
- **Physical constants**: Fundamental values for radar calculations

**Step 41: Field Name Constants**
```python
# Packet metadata dataframe field names
BAQ_MODE_FIELD_NAME = "BAQ Mode"
NUM_QUADS_FIELD_NAME = "Number of Quads"
SWATH_NUM_FIELD_NAME = "Swath Number"
SIGNAL_TYPE_FIELD_NAME = "Signal Type"
# ...many more field names
```
- **String constants**: Prevent typos in DataFrame column names
- **Maintainability**: Single location for all field name definitions
- **Consistency**: Ensures uniform naming across all modules

### Utility Functions (utilities.py) - Used as Needed

**Step 42: Sample Rate Conversion**
```python
def range_dec_to_sample_rate(rgdec_code: int) -> float:
    if rgdec_code == 0:
        return 3 * cnst.F_REF
    elif rgdec_code == 1:
        return (8 / 3) * cnst.F_REF
    # ...continuing for all valid codes
    else:
        raise Exception(f"Invalid range decimation code {rgdec_code}")
```
- **Called when needed**: Converts header codes to physical values
- **Lookup function**: Maps integer codes to actual sample rates
- **Error handling**: Validates input codes

**Step 43: Ephemeris Data Processing**
```python
def read_subcommed_data(df: pd.DataFrame) -> pd.DataFrame:
    index_col = cnst.SUBCOM_ANC_DATA_WORD_INDEX_FIELD_NAME
    data_col = cnst.SUBCOM_ANC_DATA_WORD_FIELD_NAME
    start_indices = df.index[df[index_col] == 1]
    
    # Complex multi-word reconstruction...
    x_bytes = struct.pack(">HHHH", d[0], d[1], d[2], d[3])
    x = np.frombuffer(x_bytes, dtype=dbl_type)[0]
```
- **Called by Level0File**: When ephemeris property is accessed
- **Multi-word reconstruction**: Combines 16-bit words into larger precision values
- **Satellite state**: Position, velocity, and attitude information

---

## Complete Sequential Execution Flow

### Summary of File Execution Order

**Step 44: Complete Call Chain**

1. **User imports package** → `__init__.py` executes
2. **User creates Level0File** → `l0file.py` constructor runs
3. **Level0File creates Level0Decoder** → `l0decoder.py` constructor runs  
4. **Level0File calls decode_metadata()** → `l0decoder.py` processes all headers
5. **For each packet header** → `_headers.py` functions parse binary data
6. **User calls get_burst_data()** → `l0file.py` method executes
7. **get_burst_data calls decode_packets()** → `l0decoder.py` processes payloads
8. **For each packet payload** → `_user_data_decoder.py` dispatches processing
9. **If BAQ mode 0** → `_bypass_decoder.py` extracts 10-bit samples
10. **If BAQ mode 12-14** → `_fdbaq_decoder.py` performs Huffman decoding
11. **FDBAQ creates SampleCodes** → `_sample_code.py` objects store codes
12. **Codes converted to values** → `_sample_value_reconstruction.py` processes codes
13. **Reconstruction uses tables** → `_lookup_tables.py` provides values
14. **Throughout: constants and utilities** → `constants.py` and `utilities.py` support operations

**Step 45: Data Flow Summary**

```
Raw File → Headers → Metadata DataFrame → Burst Organization
                                              ↓
User Request → Packet Selection → Payload Bytes → Decoder Dispatch
                                                        ↓
          ┌─ BAQ 0 → BypassDecoder → Direct 10-bit extraction
          │
          └─ BAQ 12-14 → FDBAQDecoder → Huffman → SampleCodes → Reconstruction → Values
                                                                      ↓
All Paths → Complex I/Q Assembly → NumPy Array → User Application
```

This sequential documentation follows the exact order of file execution, from the initial package import through the complete processing pipeline, showing how each file builds upon the previous ones to transform raw Sentinel-1 satellite data into usable radar samples.