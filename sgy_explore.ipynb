{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba36eea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBUST ANALYSIS FOR DAT_0023.SGY\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE SEG-Y DIAGNOSTIC REPORT\n",
      "======================================================================\n",
      "=== READING TEXTUAL HEADER (ROBUST MODE) ===\n",
      "Best encoding: Windows-1252\n",
      "Printable characters: 3167/3200 (99.0%)\n",
      "First readable lines:\n",
      "  C01 This SEG-Y format data file was generated by Prism Mobile software. (c) Radar Sy\n",
      "  C02 stems, Inc. (www.radsys.lv)\n",
      "The SEG-Y format is intended for seismic (not geora\n",
      "  C03 dar) data.\n",
      "Due to this fact, some lucks of correspondence to SEG-Y format occur\n",
      "  C04 :\n",
      "1. The SEG-Y EDCBIC Reel Header filled by this textual information.\n",
      "2. All t\n",
      "  C05 ime-based values represented in PICOseconds (not in MICROseconds!).\n",
      "3. Relative\n",
      "\n",
      "=== READING BINARY HEADER (ROBUST MODE) ===\n",
      "Binary Header Analysis:\n",
      "  SEG-Y Revision: 0\n",
      "  Data Format: 0 - INVALID - Not defined in SEG-Y standard\n",
      "  Samples per trace: 0\n",
      "  Sample interval: 256 microseconds\n",
      "  Trace sorting: 0 - UNKNOWN/INVALID\n",
      "\n",
      "⚠️ Issues detected:\n",
      "    - Invalid data format code: 0 - INVALID - Not defined in SEG-Y standard\n",
      "    - Invalid samples per trace: 0\n",
      "\n",
      "=== FILE STRUCTURE ANALYSIS (SAFE MODE) ===\n",
      "File size: 5,883,760 bytes (5.61 MB)\n",
      "Header size: 3,600 bytes\n",
      "\n",
      "⚠️ Structure issues:\n",
      "    - Cannot analyze traces: samples_per_trace is zero or negative\n",
      "\n",
      "=== ATTEMPTING TO READ TRACES (MAX 5 ATTEMPTS) ===\n",
      "\n",
      "=== FILE STRUCTURE ANALYSIS (SAFE MODE) ===\n",
      "File size: 5,883,760 bytes (5.61 MB)\n",
      "Header size: 3,600 bytes\n",
      "\n",
      "⚠️ Structure issues:\n",
      "    - Cannot analyze traces: samples_per_trace is zero or negative\n",
      "❌ Cannot read traces due to invalid header information\n",
      "\n",
      "==================================================\n",
      "DIAGNOSTIC SUMMARY\n",
      "==================================================\n",
      "File: DAT_0023.SGY\n",
      "Size: 5,883,760 bytes (5.61 MB)\n",
      "SEG-Y Revision: 0\n",
      "Data Format: 0 - INVALID - Not defined in SEG-Y standard\n",
      "Samples per Trace: 0\n",
      "Sample Interval: 256 µs\n",
      "⚠️  2 ISSUE(S) DETECTED:\n",
      "  1. Invalid data format code: 0 - INVALID - Not defined in SEG-Y standard\n",
      "  2. Invalid samples per trace: 0\n",
      "\n",
      "==============================\n",
      "SUMMARY TABLE\n",
      "==============================\n",
      "         Property                                       Value\n",
      "      File Status                                  ✅ Readable\n",
      "        File Size                             5,883,760 bytes\n",
      "   SEG-Y Revision                                           0\n",
      "      Data Format 0 - INVALID - Not defined in SEG-Y standard\n",
      "Samples per Trace                                           0\n",
      "  Sample Interval                                      256 µs\n",
      "     Total Issues                                           2\n",
      "      Traces Read                                           0\n",
      "\n",
      "==============================\n",
      "RECOMMENDED ACTIONS\n",
      "==============================\n",
      "This file has significant issues:\n",
      "  • Invalid data format code: 0 - INVALID - Not defined in SEG-Y standard\n",
      "  • Invalid samples per trace: 0\n",
      "\n",
      "Recommendations:\n",
      "  1. Check if this is actually a SEG-Y file\n",
      "  2. Try different SEG-Y reading software\n",
      "  3. Contact the data provider for format specifications\n",
      "  6. Try opening with specialized geophysics software (OpendTect, etc.)\n",
      "  7. Check if file uses proprietary or non-standard SEG-Y variant\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ROBUST SEG-Y ANALYZER FOR PROBLEMATIC FILES LIKE DAT_0023.SGY\n",
    "# =============================================================================\n",
    "\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class RobustSEGYVerifier:\n",
    "    \"\"\"\n",
    "    Robust SEG-Y file analyzer that handles corrupted/non-standard files\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath: str):\n",
    "        \"\"\"Initialize with SEG-Y file path\"\"\"\n",
    "        self.filepath = Path(filepath)\n",
    "        self.file_size = self.filepath.stat().st_size if self.filepath.exists() else 0\n",
    "        \n",
    "        # SEG-Y format constants\n",
    "        self.TEXTUAL_HEADER_SIZE = 3200\n",
    "        self.BINARY_HEADER_SIZE = 400\n",
    "        self.TRACE_HEADER_SIZE = 240\n",
    "        \n",
    "        # Data format codes and their properties (including invalid ones)\n",
    "        self.DATA_FORMATS = {\n",
    "            0: {'name': 'INVALID - Not defined in SEG-Y standard', 'size': 0, 'type': 'invalid'},\n",
    "            1: {'name': '4-byte IBM floating point', 'size': 4, 'type': 'ibm_float'},\n",
    "            2: {'name': '4-byte two\\'s complement integer', 'size': 4, 'type': 'int32'},\n",
    "            3: {'name': '2-byte two\\'s complement integer', 'size': 2, 'type': 'int16'},\n",
    "            4: {'name': '4-byte fixed point with gain', 'size': 4, 'type': 'fixed_point'},\n",
    "            5: {'name': '4-byte IEEE floating point', 'size': 4, 'type': 'float32'},\n",
    "            6: {'name': '8-byte IEEE floating point', 'size': 8, 'type': 'float64'},\n",
    "            7: {'name': 'RESERVED - Not used', 'size': 0, 'type': 'invalid'},\n",
    "            8: {'name': '1-byte two\\'s complement integer', 'size': 1, 'type': 'int8'}\n",
    "        }\n",
    "        \n",
    "        # Trace identification codes\n",
    "        self.TRACE_ID_CODES = {\n",
    "            1: 'Seismic data',\n",
    "            2: 'Dead trace',\n",
    "            3: 'Dummy trace',\n",
    "            4: 'Time break',\n",
    "            5: 'Uphole',\n",
    "            6: 'Sweep',\n",
    "            7: 'Timing',\n",
    "            8: 'Water break'\n",
    "        }\n",
    "        \n",
    "        # Trace sorting codes\n",
    "        self.SORTING_CODES = {\n",
    "            0: 'UNKNOWN/INVALID',\n",
    "            1: 'As recorded (field format)',\n",
    "            2: 'CDP ensemble',\n",
    "            3: 'Single fold continuous profile',\n",
    "            4: 'Horizontally stacked',\n",
    "            5: 'Common source point',\n",
    "            6: 'Common receiver point',\n",
    "            7: 'Common offset point',\n",
    "            8: 'Common mid-point',\n",
    "            9: 'Common conversion point'\n",
    "        }\n",
    "        \n",
    "        # Initialize data containers\n",
    "        self.textual_header = None\n",
    "        self.binary_header = {}\n",
    "        self.trace_headers = []\n",
    "        self.traces_data = []\n",
    "        self.file_issues = []\n",
    "    \n",
    "    def decode_textual_header_robust(self, textual_bytes: bytes) -> tuple:\n",
    "        \"\"\"Robust textual header decoding with multiple encoding attempts\"\"\"\n",
    "        encodings_to_try = [\n",
    "            ('cp500', 'IBM EBCDIC'),\n",
    "            ('cp037', 'IBM EBCDIC US'),\n",
    "            ('ascii', 'ASCII'),\n",
    "            ('utf-8', 'UTF-8'),\n",
    "            ('latin1', 'Latin-1'),\n",
    "            ('cp1252', 'Windows-1252')\n",
    "        ]\n",
    "        \n",
    "        best_result = None\n",
    "        best_encoding = None\n",
    "        max_printable = 0\n",
    "        \n",
    "        for encoding, name in encodings_to_try:\n",
    "            try:\n",
    "                decoded = textual_bytes.decode(encoding)\n",
    "                # Count printable characters\n",
    "                printable_count = sum(1 for c in decoded if c.isprintable() or c.isspace())\n",
    "                \n",
    "                if printable_count > max_printable:\n",
    "                    max_printable = printable_count\n",
    "                    best_result = decoded\n",
    "                    best_encoding = name\n",
    "                    \n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        if best_result is None:\n",
    "            # Last resort: decode with errors='replace'\n",
    "            best_result = textual_bytes.decode('utf-8', errors='replace')\n",
    "            best_encoding = 'UTF-8 (with replacements)'\n",
    "        \n",
    "        return best_result, best_encoding, max_printable\n",
    "    \n",
    "    def read_textual_header(self) -> str:\n",
    "        \"\"\"Read and decode textual file header with robust error handling\"\"\"\n",
    "        print(\"=== READING TEXTUAL HEADER (ROBUST MODE) ===\")\n",
    "        \n",
    "        with open(self.filepath, 'rb') as f:\n",
    "            textual_bytes = f.read(self.TEXTUAL_HEADER_SIZE)\n",
    "        \n",
    "        decoded_text, encoding_used, printable_count = self.decode_textual_header_robust(textual_bytes)\n",
    "        \n",
    "        print(f\"Best encoding: {encoding_used}\")\n",
    "        print(f\"Printable characters: {printable_count}/{len(decoded_text)} ({100*printable_count/len(decoded_text):.1f}%)\")\n",
    "        \n",
    "        # Parse into 40 lines of 80 characters\n",
    "        lines = []\n",
    "        for i in range(40):\n",
    "            start = i * 80\n",
    "            end = start + 80\n",
    "            line = decoded_text[start:end]\n",
    "            # Clean up non-printable characters for display\n",
    "            clean_line = ''.join(c if c.isprintable() or c.isspace() else '?' for c in line)\n",
    "            lines.append(f\"C{i+1:02d} {clean_line}\")\n",
    "        \n",
    "        self.textual_header = '\\n'.join(lines)\n",
    "        \n",
    "        # Show first few readable lines\n",
    "        print(\"First readable lines:\")\n",
    "        readable_lines = 0\n",
    "        for i, line in enumerate(lines):\n",
    "            clean_content = line[4:].strip()  # Remove \"Cxx \" prefix\n",
    "            if clean_content and not all(c in '? ' for c in clean_content):\n",
    "                print(f\"  {line}\")\n",
    "                readable_lines += 1\n",
    "                if readable_lines >= 5:\n",
    "                    break\n",
    "        \n",
    "        if readable_lines == 0:\n",
    "            print(\"  [No clearly readable text found - file may be corrupted]\")\n",
    "            self.file_issues.append(\"Textual header appears corrupted or uses unsupported encoding\")\n",
    "        \n",
    "        return self.textual_header\n",
    "    \n",
    "    def read_binary_header_safe(self) -> Dict:\n",
    "        \"\"\"Read binary header with comprehensive error checking\"\"\"\n",
    "        print(\"\\n=== READING BINARY HEADER (ROBUST MODE) ===\")\n",
    "        \n",
    "        with open(self.filepath, 'rb') as f:\n",
    "            f.seek(self.TEXTUAL_HEADER_SIZE)\n",
    "            binary_bytes = f.read(self.BINARY_HEADER_SIZE)\n",
    "        \n",
    "        if len(binary_bytes) < self.BINARY_HEADER_SIZE:\n",
    "            self.file_issues.append(f\"Binary header truncated: {len(binary_bytes)} bytes (expected {self.BINARY_HEADER_SIZE})\")\n",
    "            return {}\n",
    "        \n",
    "        # Parse fields with error handling\n",
    "        try:\n",
    "            header_fields = {\n",
    "                'job_id': struct.unpack('>i', binary_bytes[12:16])[0],\n",
    "                'line_number': struct.unpack('>i', binary_bytes[16:20])[0],\n",
    "                'reel_number': struct.unpack('>i', binary_bytes[20:24])[0],\n",
    "                'traces_per_ensemble': struct.unpack('>h', binary_bytes[22:24])[0],\n",
    "                'aux_traces_per_ensemble': struct.unpack('>h', binary_bytes[24:26])[0],\n",
    "                'sample_interval': struct.unpack('>h', binary_bytes[26:28])[0],\n",
    "                'sample_interval_original': struct.unpack('>h', binary_bytes[28:30])[0],\n",
    "                'samples_per_trace': struct.unpack('>h', binary_bytes[30:32])[0],\n",
    "                'samples_per_trace_original': struct.unpack('>h', binary_bytes[32:34])[0],\n",
    "                'data_sample_format': struct.unpack('>h', binary_bytes[34:36])[0],\n",
    "                'ensemble_fold': struct.unpack('>h', binary_bytes[36:38])[0],\n",
    "                'trace_sorting_code': struct.unpack('>h', binary_bytes[38:40])[0],\n",
    "                'vertical_sum_code': struct.unpack('>h', binary_bytes[40:42])[0],\n",
    "                'segy_revision': struct.unpack('>h', binary_bytes[64:66])[0],\n",
    "                'fixed_length_trace_flag': struct.unpack('>h', binary_bytes[66:68])[0],\n",
    "                'extended_textual_headers': struct.unpack('>h', binary_bytes[68:70])[0]\n",
    "            }\n",
    "        except struct.error as e:\n",
    "            self.file_issues.append(f\"Binary header parsing error: {e}\")\n",
    "            return {}\n",
    "        \n",
    "        self.binary_header = header_fields\n",
    "        \n",
    "        # Validate critical fields\n",
    "        issues = []\n",
    "        \n",
    "        # Check data format\n",
    "        format_code = header_fields['data_sample_format']\n",
    "        if format_code not in self.DATA_FORMATS:\n",
    "            issues.append(f\"Unknown data format code: {format_code}\")\n",
    "        elif self.DATA_FORMATS[format_code]['size'] == 0:\n",
    "            issues.append(f\"Invalid data format code: {format_code} - {self.DATA_FORMATS[format_code]['name']}\")\n",
    "        \n",
    "        # Check samples per trace\n",
    "        if header_fields['samples_per_trace'] <= 0:\n",
    "            issues.append(f\"Invalid samples per trace: {header_fields['samples_per_trace']}\")\n",
    "        \n",
    "        # Check sample interval\n",
    "        if header_fields['sample_interval'] <= 0:\n",
    "            issues.append(f\"Invalid sample interval: {header_fields['sample_interval']}\")\n",
    "        \n",
    "        # Check SEG-Y revision\n",
    "        if header_fields['segy_revision'] not in [0, 1, 2]:\n",
    "            issues.append(f\"Unknown SEG-Y revision: {header_fields['segy_revision']}\")\n",
    "        \n",
    "        self.file_issues.extend(issues)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"Binary Header Analysis:\")\n",
    "        print(f\"  SEG-Y Revision: {header_fields['segy_revision']}\")\n",
    "        print(f\"  Data Format: {format_code} - {self.DATA_FORMATS.get(format_code, {}).get('name', 'Unknown')}\")\n",
    "        print(f\"  Samples per trace: {header_fields['samples_per_trace']}\")\n",
    "        print(f\"  Sample interval: {header_fields['sample_interval']} microseconds\")\n",
    "        print(f\"  Trace sorting: {header_fields['trace_sorting_code']} - {self.SORTING_CODES.get(header_fields['trace_sorting_code'], 'Unknown')}\")\n",
    "        \n",
    "        if issues:\n",
    "            print(\"\\n⚠️ Issues detected:\")\n",
    "            for issue in issues:\n",
    "                print(f\"    - {issue}\")\n",
    "        \n",
    "        return header_fields\n",
    "    \n",
    "    def analyze_file_structure_safe(self) -> Dict:\n",
    "        \"\"\"Analyze file structure with safety checks for invalid data\"\"\"\n",
    "        print(\"\\n=== FILE STRUCTURE ANALYSIS (SAFE MODE) ===\")\n",
    "        \n",
    "        if not self.binary_header:\n",
    "            self.read_binary_header_safe()\n",
    "        \n",
    "        analysis = {\n",
    "            'file_size_actual': self.file_size,\n",
    "            'header_size': self.TEXTUAL_HEADER_SIZE + self.BINARY_HEADER_SIZE,\n",
    "            'can_analyze_traces': False,\n",
    "            'issues': []\n",
    "        }\n",
    "        \n",
    "        # Check if we can analyze trace structure\n",
    "        samples_per_trace = self.binary_header.get('samples_per_trace', 0)\n",
    "        format_code = self.binary_header.get('data_sample_format', 0)\n",
    "        \n",
    "        if samples_per_trace <= 0:\n",
    "            analysis['issues'].append(\"Cannot analyze traces: samples_per_trace is zero or negative\")\n",
    "        elif format_code not in self.DATA_FORMATS:\n",
    "            analysis['issues'].append(f\"Cannot analyze traces: unknown format code {format_code}\")\n",
    "        elif self.DATA_FORMATS[format_code]['size'] == 0:\n",
    "            analysis['issues'].append(f\"Cannot analyze traces: invalid format code {format_code}\")\n",
    "        else:\n",
    "            # We can analyze trace structure\n",
    "            analysis['can_analyze_traces'] = True\n",
    "            \n",
    "            bytes_per_sample = self.DATA_FORMATS[format_code]['size']\n",
    "            trace_data_size = samples_per_trace * bytes_per_sample\n",
    "            trace_total_size = self.TRACE_HEADER_SIZE + trace_data_size\n",
    "            \n",
    "            remaining_bytes = self.file_size - analysis['header_size']\n",
    "            estimated_traces = remaining_bytes // trace_total_size if trace_total_size > 0 else 0\n",
    "            expected_file_size = analysis['header_size'] + (estimated_traces * trace_total_size)\n",
    "            \n",
    "            analysis.update({\n",
    "                'trace_data_size': trace_data_size,\n",
    "                'trace_total_size': trace_total_size,\n",
    "                'bytes_per_sample': bytes_per_sample,\n",
    "                'estimated_traces': estimated_traces,\n",
    "                'file_size_expected': expected_file_size,\n",
    "                'size_difference': self.file_size - expected_file_size\n",
    "            })\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"File size: {self.file_size:,} bytes ({self.file_size/1024/1024:.2f} MB)\")\n",
    "        print(f\"Header size: {analysis['header_size']:,} bytes\")\n",
    "        \n",
    "        if analysis['can_analyze_traces']:\n",
    "            print(f\"Expected file size: {analysis['file_size_expected']:,} bytes\")\n",
    "            print(f\"Size difference: {analysis['size_difference']:,} bytes\")\n",
    "            print(f\"Estimated traces: {analysis['estimated_traces']:,}\")\n",
    "            print(f\"Bytes per trace: {analysis['trace_total_size']:,}\")\n",
    "            \n",
    "            if abs(analysis['size_difference']) > 1024:  # More than 1KB difference\n",
    "                analysis['issues'].append(f\"File size mismatch: {analysis['size_difference']:,} bytes\")\n",
    "        \n",
    "        if analysis['issues']:\n",
    "            print(\"\\n⚠️ Structure issues:\")\n",
    "            for issue in analysis['issues']:\n",
    "                print(f\"    - {issue}\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def attempt_trace_reading(self, max_attempts: int = 5) -> List[Dict]:\n",
    "        \"\"\"Attempt to read traces even from problematic files\"\"\"\n",
    "        print(f\"\\n=== ATTEMPTING TO READ TRACES (MAX {max_attempts} ATTEMPTS) ===\")\n",
    "        \n",
    "        if not self.binary_header:\n",
    "            self.read_binary_header_safe()\n",
    "        \n",
    "        structure = self.analyze_file_structure_safe()\n",
    "        \n",
    "        if not structure['can_analyze_traces']:\n",
    "            print(\"❌ Cannot read traces due to invalid header information\")\n",
    "            return []\n",
    "        \n",
    "        # Try to read traces\n",
    "        header_offset = self.TEXTUAL_HEADER_SIZE + self.BINARY_HEADER_SIZE\n",
    "        trace_total_size = structure['trace_total_size']\n",
    "        \n",
    "        successful_traces = []\n",
    "        \n",
    "        for i in range(min(max_attempts, structure.get('estimated_traces', 0))):\n",
    "            try:\n",
    "                trace_offset = header_offset + (i * trace_total_size)\n",
    "                \n",
    "                # Check if we have enough bytes left\n",
    "                if trace_offset + trace_total_size > self.file_size:\n",
    "                    print(f\"⚠️ Trace {i+1}: Not enough data remaining in file\")\n",
    "                    break\n",
    "                \n",
    "                # Try to read trace header\n",
    "                with open(self.filepath, 'rb') as f:\n",
    "                    f.seek(trace_offset)\n",
    "                    trace_header_bytes = f.read(self.TRACE_HEADER_SIZE)\n",
    "                \n",
    "                if len(trace_header_bytes) < self.TRACE_HEADER_SIZE:\n",
    "                    print(f\"⚠️ Trace {i+1}: Truncated trace header\")\n",
    "                    break\n",
    "                \n",
    "                # Parse basic trace header fields\n",
    "                trace_header = {\n",
    "                    'trace_seq_line': struct.unpack('>i', trace_header_bytes[0:4])[0],\n",
    "                    'samples_in_trace': struct.unpack('>h', trace_header_bytes[114:116])[0],\n",
    "                    'sample_interval_trace': struct.unpack('>h', trace_header_bytes[116:118])[0],\n",
    "                }\n",
    "                \n",
    "                # Validate trace header\n",
    "                if trace_header['samples_in_trace'] != self.binary_header['samples_per_trace']:\n",
    "                    print(f\"⚠️ Trace {i+1}: Sample count mismatch ({trace_header['samples_in_trace']} vs {self.binary_header['samples_per_trace']})\")\n",
    "                \n",
    "                # Try to read trace data\n",
    "                data_offset = trace_offset + self.TRACE_HEADER_SIZE\n",
    "                bytes_to_read = structure['trace_data_size']\n",
    "                \n",
    "                with open(self.filepath, 'rb') as f:\n",
    "                    f.seek(data_offset)\n",
    "                    data_bytes = f.read(bytes_to_read)\n",
    "                \n",
    "                if len(data_bytes) < bytes_to_read:\n",
    "                    print(f\"⚠️ Trace {i+1}: Truncated trace data ({len(data_bytes)} vs {bytes_to_read} bytes)\")\n",
    "                    break\n",
    "                \n",
    "                # Create dummy trace data for invalid formats\n",
    "                format_code = self.binary_header['data_sample_format']\n",
    "                if self.DATA_FORMATS[format_code]['size'] == 0:\n",
    "                    print(f\"⚠️ Trace {i+1}: Cannot decode data due to invalid format code\")\n",
    "                    # Create synthetic data for analysis\n",
    "                    trace_data = np.zeros(self.binary_header['samples_per_trace'])\n",
    "                else:\n",
    "                    # Try to interpret data (simplified)\n",
    "                    samples = self.binary_header['samples_per_trace']\n",
    "                    trace_data = np.frombuffer(data_bytes[:samples*4], dtype='>f4')[:samples]\n",
    "                \n",
    "                successful_traces.append({\n",
    "                    'header': trace_header,\n",
    "                    'data': trace_data,\n",
    "                    'trace_number': i + 1\n",
    "                })\n",
    "                \n",
    "                print(f\"✅ Trace {i+1}: Successfully read ({len(trace_data)} samples)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Trace {i+1}: Error reading trace - {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nSuccessfully read {len(successful_traces)} traces\")\n",
    "        \n",
    "        self.trace_headers = [t['header'] for t in successful_traces]\n",
    "        self.traces_data = [t['data'] for t in successful_traces]\n",
    "        \n",
    "        return successful_traces\n",
    "    \n",
    "    def generate_diagnostic_report(self) -> Dict:\n",
    "        \"\"\"Generate comprehensive diagnostic report for problematic files\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPREHENSIVE SEG-Y DIAGNOSTIC REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        report = {\n",
    "            'file_info': {\n",
    "                'filepath': str(self.filepath),\n",
    "                'file_size': self.file_size,\n",
    "                'exists': self.filepath.exists()\n",
    "            },\n",
    "            'issues': self.file_issues,\n",
    "            'analysis_results': {}\n",
    "        }\n",
    "        \n",
    "        if not self.filepath.exists():\n",
    "            print(\"❌ File does not exist!\")\n",
    "            return report\n",
    "        \n",
    "        try:\n",
    "            # Read headers\n",
    "            self.read_textual_header()\n",
    "            self.read_binary_header_safe()\n",
    "            \n",
    "            # Analyze structure\n",
    "            structure = self.analyze_file_structure_safe()\n",
    "            report['structure'] = structure\n",
    "            \n",
    "            # Try to read traces\n",
    "            traces = self.attempt_trace_reading(5)\n",
    "            \n",
    "            # Summary\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"DIAGNOSTIC SUMMARY\")\n",
    "            print(f\"{'='*50}\")\n",
    "            print(f\"File: {self.filepath.name}\")\n",
    "            print(f\"Size: {self.file_size:,} bytes ({self.file_size/1024/1024:.2f} MB)\")\n",
    "            \n",
    "            if self.binary_header:\n",
    "                print(f\"SEG-Y Revision: {self.binary_header.get('segy_revision', 'Unknown')}\")\n",
    "                print(f\"Data Format: {self.binary_header.get('data_sample_format')} - {self.DATA_FORMATS.get(self.binary_header.get('data_sample_format', 0), {}).get('name', 'Unknown')}\")\n",
    "                print(f\"Samples per Trace: {self.binary_header.get('samples_per_trace', 0)}\")\n",
    "                print(f\"Sample Interval: {self.binary_header.get('sample_interval', 0)} µs\")\n",
    "            \n",
    "            if structure.get('can_analyze_traces'):\n",
    "                print(f\"Estimated Traces: {structure.get('estimated_traces', 0):,}\")\n",
    "                print(f\"Successfully Read Traces: {len(traces)}\")\n",
    "            \n",
    "            total_issues = len(self.file_issues)\n",
    "            if total_issues == 0:\n",
    "                print(\"✅ NO ISSUES DETECTED\")\n",
    "            else:\n",
    "                print(f\"⚠️  {total_issues} ISSUE(S) DETECTED:\")\n",
    "                for i, issue in enumerate(self.file_issues, 1):\n",
    "                    print(f\"  {i}. {issue}\")\n",
    "            \n",
    "            report['summary'] = {\n",
    "                'total_issues': total_issues,\n",
    "                'traces_read': len(traces),\n",
    "                'can_analyze': structure.get('can_analyze_traces', False)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ ANALYSIS FAILED - Error: {str(e)}\")\n",
    "            report['error'] = str(e)\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def create_summary_table(self) -> pd.DataFrame:\n",
    "        \"\"\"Create a summary table of findings\"\"\"\n",
    "        if not self.binary_header:\n",
    "            return pd.DataFrame([{\"Property\": \"Status\", \"Value\": \"No data available\"}])\n",
    "        \n",
    "        data = [\n",
    "            {\"Property\": \"File Status\", \"Value\": \"✅ Readable\" if self.filepath.exists() else \"❌ Not found\"},\n",
    "            {\"Property\": \"File Size\", \"Value\": f\"{self.file_size:,} bytes\"},\n",
    "            {\"Property\": \"SEG-Y Revision\", \"Value\": self.binary_header.get('segy_revision', 'Unknown')},\n",
    "            {\"Property\": \"Data Format\", \"Value\": f\"{self.binary_header.get('data_sample_format')} - {self.DATA_FORMATS.get(self.binary_header.get('data_sample_format', 0), {}).get('name', 'Unknown')}\"},\n",
    "            {\"Property\": \"Samples per Trace\", \"Value\": self.binary_header.get('samples_per_trace', 0)},\n",
    "            {\"Property\": \"Sample Interval\", \"Value\": f\"{self.binary_header.get('sample_interval', 0)} µs\"},\n",
    "            {\"Property\": \"Total Issues\", \"Value\": len(self.file_issues)},\n",
    "            {\"Property\": \"Traces Read\", \"Value\": len(self.traces_data)},\n",
    "        ]\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS FOR YOUR SPECIFIC DAT_0023.SGY FILE\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_dat_0023_robust():\n",
    "    \"\"\"\n",
    "    Robust analysis specifically for your problematic DAT_0023.SGY file\n",
    "    \"\"\"\n",
    "    print(\"ROBUST ANALYSIS FOR DAT_0023.SGY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Initialize robust verifier\n",
    "    verifier = RobustSEGYVerifier(\"DAT_0023.SGY\")\n",
    "    \n",
    "    # Run diagnostic analysis\n",
    "    report = verifier.generate_diagnostic_report()\n",
    "    \n",
    "    # Create summary table\n",
    "    summary = verifier.create_summary_table()\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(f\"{'='*30}\")\n",
    "    print(summary.to_string(index=False))\n",
    "    \n",
    "    # Show recommended actions\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(\"RECOMMENDED ACTIONS\")\n",
    "    print(f\"{'='*30}\")\n",
    "    \n",
    "    if verifier.file_issues:\n",
    "        print(\"This file has significant issues:\")\n",
    "        for issue in verifier.file_issues:\n",
    "            print(f\"  • {issue}\")\n",
    "        \n",
    "        print(\"\\nRecommendations:\")\n",
    "        if \"Invalid data format code: 0\" in ' '.join(verifier.file_issues):\n",
    "            print(\"  1. Check if this is actually a SEG-Y file\")\n",
    "            print(\"  2. Try different SEG-Y reading software\")\n",
    "            print(\"  3. Contact the data provider for format specifications\")\n",
    "        \n",
    "        if \"samples_per_trace is zero\" in ' '.join(verifier.file_issues):\n",
    "            print(\"  4. File may be corrupted or incomplete\")\n",
    "            print(\"  5. Check if file transfer was successful\")\n",
    "        \n",
    "        print(\"  6. Try opening with specialized geophysics software (OpendTect, etc.)\")\n",
    "        print(\"  7. Check if file uses proprietary or non-standard SEG-Y variant\")\n",
    "    else:\n",
    "        print(\"✅ File appears to be valid SEG-Y format\")\n",
    "    \n",
    "    return verifier, report\n",
    "\n",
    "# Run the robust analysis\n",
    "if __name__ == \"__main__\":\n",
    "    verifier, report = analyze_dat_0023_robust()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942324c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
